{"cells":[{"cell_type":"markdown","id":"YPB2EK9FEiMp","metadata":{"id":"YPB2EK9FEiMp"},"source":["# Warning :\n","# Do \"File -> Save a copy in Drive\" before you start modifying the notebook, otherwise your modifications will not be saved."]},{"cell_type":"markdown","id":"_UFwj3KNufA4","metadata":{"id":"_UFwj3KNufA4"},"source":["# BERT for Sentiment Analysis\n","# A) Compute BERT embedding for each review => CLS token"]},{"cell_type":"code","execution_count":null,"id":"IlFyycx9qLTP","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707493805037,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"IlFyycx9qLTP"},"outputs":[],"source":["#! pip install transformers"]},{"cell_type":"code","execution_count":null,"id":"6e14ce48","metadata":{"executionInfo":{"elapsed":10114,"status":"ok","timestamp":1707493815148,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"6e14ce48"},"outputs":[],"source":["import transformers\n","import tensorflow as tf"]},{"cell_type":"markdown","id":"BtKn-7auvbsh","metadata":{"id":"BtKn-7auvbsh"},"source":["# Downloading large review movie dataset (25000 reviews)"]},{"cell_type":"code","execution_count":null,"id":"a5i1H9--qZsC","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3126,"status":"ok","timestamp":1707493818265,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"a5i1H9--qZsC","outputId":"6677c29f-e1e5-4c0a-e8b5-41d5e4dfca72"},"outputs":[],"source":["!wget https://thome.isir.upmc.fr/classes/RITAL/json_pol.json"]},{"cell_type":"code","execution_count":null,"id":"6c40511e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1707493818636,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"6c40511e","outputId":"37cad836-237c-4b26-fc60-1fda01c167cd"},"outputs":[],"source":["import json\n","from collections import Counter\n","\n","# Loading json\n","file = './json_pol.json'\n","with open(file,encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","\n","# Quick Check\n","counter = Counter((x[1] for x in data))\n","print(\"Number of reviews : \", len(data))\n","print(\"----> # of positive : \", counter[1])\n","print(\"----> # of negative : \", counter[0])\n","print(\"\")\n","print(data[0])"]},{"cell_type":"markdown","id":"0dsRcmntwfOH","metadata":{"id":"0dsRcmntwfOH"},"source":["# Getting the Tokenizer"]},{"cell_type":"code","execution_count":null,"id":"4381e234","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1505,"status":"ok","timestamp":1707493820134,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"4381e234","outputId":"99f3561e-8e8e-4348-fdc5-6b7a2d06238f"},"outputs":[],"source":["model_name = \"rttl-ai/bert-base-uncased-yelp-reviews\"\n","\n","\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n"]},{"cell_type":"markdown","id":"GPTSFflDwkoh","metadata":{"id":"GPTSFflDwkoh"},"source":["# Experiment the Tokenizer on the first train review"]},{"cell_type":"code","execution_count":null,"id":"ddc98b0c","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1707493820135,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"ddc98b0c"},"outputs":[],"source":["maxL = 512 # Max length of the sequence\n","\n","string_tokenized = tokenizer.encode_plus(data[0][0], return_tensors=\"pt\",\n","                                        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n","                            max_length=maxL,  # set max length\n","                            truncation=True,  # truncate longer messages\n","                            #pad_to_max_length=True\n","                            padding='max_length',  # add padding\n","                            return_attention_mask=True)"]},{"cell_type":"markdown","id":"v910nrNVx33z","metadata":{"id":"v910nrNVx33z"},"source":["The output of the tokenizer string_tokenized (class BatchEncoding) returns two elements:\n","\n","\n","*   string_tokenized['input_ids']: the index of each token in the dictionary\n","*   string_tokenized['attention_mask']: a binary mask (0 to ignore the token, 1 to consider it). This is because we need tensor a fixed length and we have reviews with a variable number of words\n","\n"]},{"cell_type":"code","execution_count":null,"id":"sagULO5nx-3H","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707493820135,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"sagULO5nx-3H","outputId":"057e6e88-604d-46df-eff4-77cf2f1b7a46"},"outputs":[],"source":["print(string_tokenized['input_ids'])\n","print(string_tokenized['attention_mask'])"]},{"cell_type":"markdown","id":"hdBab99u4HNm","metadata":{"id":"hdBab99u4HNm"},"source":["# Let's tokenize the whole dataset"]},{"cell_type":"code","execution_count":null,"id":"d40ca05a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49905,"status":"ok","timestamp":1707493870034,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"d40ca05a","outputId":"0c7a7f10-63ce-4ae5-b68f-8a492423e0c5"},"outputs":[],"source":["import numpy as np\n","\n","maxL = 512\n","temb = 768\n","\n","inputs_tokens = []\n","attention_masks = []\n","\n","for i in range(len(data)):\n","    if(i%2500==0):\n","        print(i)\n","    string_tokenized = tokenizer.encode_plus(data[i][0], return_tensors=\"pt\",\n","                                        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n","                            max_length=maxL,  # set max length\n","                            truncation=True,  # truncate longer messages\n","                            #pad_to_max_length=True\n","                            padding='max_length',  # add padding\n","                            return_attention_mask=True)\n","\n","    # APPEND inputs token and input masks. YOUR CODE HERE\n","    inputs_tokens.append(string_tokenized['input_ids'])\n","    attention_masks.append(string_tokenized['attention_mask'])"]},{"cell_type":"markdown","id":"lZrSrBuS-HnW","metadata":{"id":"lZrSrBuS-HnW"},"source":["# Let's create a 'TensorDataSet' FOR THE SAMPLES where each element is a triplet composed of token word index, token mask, and label"]},{"cell_type":"code","execution_count":null,"id":"93880db1","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1707493870036,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"93880db1"},"outputs":[],"source":["import torch\n","\n","# Converting input tokens to torch tensors\n","inputs_tokens = torch.cat(inputs_tokens, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","\n","\n","# Converting labels to numpy then torch tensor\n","y = torch.zeros((len(data),))\n","for i in range(len(data)):\n","    y[i] = data[i][1]\n","#y = torch.from_numpy(y)\n","\n","from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","dataset = TensorDataset(inputs_tokens, attention_masks, y)"]},{"cell_type":"markdown","id":"TunnUo2p0FT9","metadata":{"id":"TunnUo2p0FT9"},"source":["# Lets download a BERT model for word embedding"]},{"cell_type":"code","execution_count":null,"id":"9fc3269c","metadata":{"executionInfo":{"elapsed":2542,"status":"ok","timestamp":1707493872567,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"9fc3269c"},"outputs":[],"source":["from transformers import BertForSequenceClassification\n","model = BertForSequenceClassification.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"id":"1155599b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1707493872567,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"1155599b","outputId":"bbbea24b-8c0a-4474-e3c6-64e8eb7c9723"},"outputs":[],"source":["print(model)"]},{"cell_type":"markdown","id":"23VAnnrU0QO5","metadata":{"id":"23VAnnrU0QO5"},"source":["**You can use the BERT model for directly predicting polarity.** Let us apply that on the first review which has been tokenized with string_tokenized."]},{"cell_type":"code","execution_count":null,"id":"a15ab0db","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3449,"status":"ok","timestamp":1707493875997,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"a15ab0db","outputId":"6c969864-5966-41ca-8292-897aadf10f16"},"outputs":[],"source":["# Some preliminary test\n","import torch\n","import numpy as np\n","b_input_ids = string_tokenized['input_ids']\n","b_input_mask = string_tokenized['attention_mask']\n","\n","model.eval()\n","\n","output = model(input_ids=b_input_ids,attention_mask=b_input_mask, output_hidden_states=True)\n","print(output.logits) # The output of the logit of the two classes (polarity pos/neg)\n","last_hidden_states = output.hidden_states[-1] # The last layer before the class prediction: tensor of size nBatch (1 here) x MaxL (512) x temb (768)\n","print(last_hidden_states.shape)\n","print(last_hidden_states[0,0,1:10]) # The first 10 value of the first elements (=[CLS] TOKEN)\n","print(f\" norm cls token={np.linalg.norm(last_hidden_states.detach().numpy()[0,0,:])}\")"]},{"cell_type":"code","execution_count":null,"id":"1bX5vtWb_vj8","metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1707493876000,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"1bX5vtWb_vj8"},"outputs":[],"source":["# If you need to clean GPU memory\n","#import gc\n","#gc.collect()\n","#torch.cuda.empty_cache()"]},{"cell_type":"markdown","id":"FoDmvAIO_6l8","metadata":{"id":"FoDmvAIO_6l8"},"source":["# Most important STEP: we want to extract the [CLS] representation (1st token of the last layer before logits) for each review, and store it.  "]},{"cell_type":"code","execution_count":null,"id":"90ee3dc0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":797673,"status":"ok","timestamp":1707494673648,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"90ee3dc0","outputId":"6d7d2cb2-aa5e-464c-920a-06a1d30ad447"},"outputs":[],"source":["# create DataLoaders with samplers\n","tb = int(100) # batch size\n","dataloader = DataLoader(dataset, batch_size=tb,shuffle=False) # dataloader\n","nbTrain = len(data)\n","features = np.zeros((nbTrain, temb))\n","nbatch = int(nbTrain/tb)\n","print(f\"nb batches={nbatch}\")\n","# Comuting CLS features\n","model.cuda()\n","for idx,batch in enumerate(dataloader):\n","        # Unpack this training batch from our dataloader:\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids\n","        #   [1]: attention masks\n","        #   [2]: labels\n","        if(idx%10==0):\n","            print(f\"batch {idx} / {nbatch}\")\n","        b_input_ids = batch[0].cuda()\n","        b_input_mask = batch[1].cuda()\n","        b_labels = batch[2].cuda().long()\n","\n","        with torch.no_grad():\n","            # forward propagation (evaluate model on training batch)\n","            output = model(input_ids=b_input_ids,\n","                                 attention_mask=b_input_mask,\n","                                 #labels=b_labels,\n","                               output_hidden_states=True)\n","            last_hidden_states = output.hidden_states[-1] # WARNING: it is now a batch of size tbatch x nToken x embsize\n","            features[idx*tb:idx*tb+tb,:] = last_hidden_states.detach().cpu().numpy()[:,0,:] # YOUR CODE HERE. Think in applying .detach().cpu().numpy()\n"]},{"cell_type":"markdown","id":"AReISeiZIo9U","metadata":{"id":"AReISeiZIo9U"},"source":["# Now save the embedding of each review into disk!"]},{"cell_type":"code","execution_count":null,"id":"deb81156-d885-4999-a6e7-2d8149173174","metadata":{"executionInfo":{"elapsed":365,"status":"ok","timestamp":1707494674005,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"deb81156-d885-4999-a6e7-2d8149173174"},"outputs":[],"source":["# Saving the features and labels\n","import pickle\n","# Open a file and use dump()\n","with open('data.pkl', 'wb') as file:\n","    # A new file will be created\n","    pickle.dump([features,y], file)"]},{"cell_type":"code","execution_count":null,"id":"67f5f29e","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1707494674007,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"67f5f29e"},"outputs":[],"source":["import pickle\n","\n","# Open the file in binary mode\n","with open('data.pkl', 'rb') as file:\n","    # Call load method to deserialze\n","    [features, y] = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"id":"4104eea1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1707494674007,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"4104eea1","outputId":"d9e44325-241a-47e0-ae61-14347176d4fe"},"outputs":[],"source":["import numpy as np\n","print(features.shape[0])\n","print(y)\n","print(np.linalg.norm(features[10]))"]},{"cell_type":"markdown","id":"mx0eec9RIwJr","metadata":{"id":"mx0eec9RIwJr"},"source":["# B) Train a logistic regression model on top of extracted embeddings. Conclude on the performances of BERT for the sentiment classification task"]},{"cell_type":"code","execution_count":null,"id":"8HuUwq23ietG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1738,"status":"ok","timestamp":1707494675738,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"8HuUwq23ietG","outputId":"91c9785d-d529-458d-e9cf-03acd806fe8f"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","np.random.seed(0)\n","\n","rs=10\n","[X_train, X_test, y_train, y_test]  = train_test_split(features, y, test_size=0.5, random_state=rs, shuffle=True)\n","\n","lr_clf = LogisticRegression()\n","lr_clf.fit(X_train, y_train)\n","print(lr_clf.score(X_test, y_test))"]},{"cell_type":"markdown","id":"lqkuF9hla7is","metadata":{"id":"lqkuF9hla7is"},"source":["# C) Fine-tuning BERT for sentiment classification"]},{"cell_type":"code","execution_count":null,"id":"_qMLN-nZ6mls","metadata":{"executionInfo":{"elapsed":439,"status":"ok","timestamp":1707494676171,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"_qMLN-nZ6mls"},"outputs":[],"source":["# We will fine-tune a smaller model\n","model_name = \"haisongzhang/roberta-tiny-cased\"\n","#model_name = \"rttl-ai/bert-base-uncased-yelp-reviews\"\n","\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n"]},{"cell_type":"markdown","id":"r3X5jj3YPOuP","metadata":{"id":"r3X5jj3YPOuP"},"source":["# Let's tokenize the whole dataset"]},{"cell_type":"code","execution_count":null,"id":"SFjlfBWx7ES_","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1707494676171,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"SFjlfBWx7ES_"},"outputs":[],"source":["maxL = 512 # Max length of the sequence\n","\n","string_tokenized = tokenizer.encode_plus(data[0][0], return_tensors=\"pt\",\n","                                        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n","                            max_length=maxL,  # set max length\n","                            truncation=True,  # truncate longer messages\n","                            #pad_to_max_length=True\n","                            padding='max_length',  # add padding\n","                            return_attention_mask=True)"]},{"cell_type":"code","execution_count":null,"id":"DqcGUTKs7J1S","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39579,"status":"ok","timestamp":1707494715741,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"DqcGUTKs7J1S","outputId":"80c70333-a083-483d-8cb9-5ff083fc7ff7"},"outputs":[],"source":["import numpy as np\n","\n","maxL = 512\n","\n","\n","inputs_tokens = []\n","attention_masks = []\n","\n","for i in range(len(data)):\n","    if(i%2500==0):\n","        print(i)\n","    string_tokenized = tokenizer.encode_plus(data[i][0], return_tensors=\"pt\",\n","                                        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n","                            max_length=maxL,  # set max length\n","                            truncation=True,  # truncate longer messages\n","                            #pad_to_max_length=True\n","                            padding='max_length',  # add padding\n","                            return_attention_mask=True)\n","\n","    inputs_tokens.append(string_tokenized['input_ids'])\n","    attention_masks.append(string_tokenized['attention_mask'])"]},{"cell_type":"markdown","id":"teetkzTLPZAH","metadata":{"id":"teetkzTLPZAH"},"source":["# Let's create 'TensorDataSets' FOR THE TRAIN/TEST SAMPLES where each element is a triplet composed of token word index, token mask, and label"]},{"cell_type":"code","execution_count":null,"id":"vCBEBN7_7Kyu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":775,"status":"ok","timestamp":1707494716505,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"vCBEBN7_7Kyu","outputId":"de36840d-4351-4c7c-c2ed-145d45c4c6e2"},"outputs":[],"source":["import torch\n","# Converting input tokens to torch tensors\n","inputs_tokens = torch.cat(inputs_tokens, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","# Converting labels to torch tensor\n","y = torch.zeros((len(data),2), dtype=torch.float)\n","for i in range(len(data)):\n","    y[i][data[i][1]] = 1\n","\n","from sklearn.model_selection import train_test_split\n","\n","np.random.seed(0)\n","rs=10\n","\n","inputs_tokens_train, inputs_tokens_test, attention_masks_train, attention_masks_test, y_train, y_test =train_test_split(inputs_tokens, attention_masks, y, test_size=0.5, random_state=rs)\n","\n","print(inputs_tokens_train.shape)\n","print(inputs_tokens_test.shape)\n","\n","print(attention_masks_train.shape)\n","print(attention_masks_test.shape)\n","\n","print(y_train.shape)\n","print(y_test.shape)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"z0tuGSAJi3Qm","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1707494716506,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"z0tuGSAJi3Qm"},"outputs":[],"source":["from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","\n","dataset_train = TensorDataset(inputs_tokens_train,  attention_masks_train, y_train)\n","dataset_test = TensorDataset(inputs_tokens_test,  attention_masks_test, y_test)"]},{"cell_type":"markdown","id":"Z7q5q3BkOglA","metadata":{"id":"Z7q5q3BkOglA"},"source":["# Lets download a BERT model for word embedding"]},{"cell_type":"code","execution_count":null,"id":"M-sFUtXVOlAK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":632,"status":"ok","timestamp":1707494717130,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"M-sFUtXVOlAK","outputId":"49764b47-2209-4aa6-e12c-4a680d26685f"},"outputs":[],"source":["from transformers import BertForSequenceClassification\n","model = BertForSequenceClassification.from_pretrained(model_name)\n","print(model)"]},{"cell_type":"markdown","id":"TvNXAu52OyZa","metadata":{"id":"TvNXAu52OyZa"},"source":["# FINE-TUNING THE MODEL"]},{"cell_type":"code","execution_count":null,"id":"GNMaLreKPszV","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1707494717132,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"GNMaLreKPszV"},"outputs":[],"source":["#import gc\n","#gc.collect()\n","#torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"id":"QXBRbzhZPyPh","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1707494717133,"user":{"displayName":"Ethan","userId":"07849915476595134169"},"user_tz":-60},"id":"QXBRbzhZPyPh"},"outputs":[],"source":["# Fonction to compute the accuracy on train/test sets\n","def accuracy(model, dataloader):\n","  model.eval()\n","  nbgood =0\n","  for idx,batch in enumerate(dataloader):\n","    b_input_ids = batch[0].cuda()\n","    b_input_mask = batch[1].cuda()\n","    b_labels = batch[2].cuda()\n","\n","    with torch.no_grad():\n","      pred = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n","      yhat = pred.logits.argmax(axis=1)\n","      ytrue = b_labels.argmax(axis=1)\n","      nbgood += (yhat==ytrue).sum()\n","\n","  acc = nbgood / 125.0\n","  return acc.item()\n"]},{"cell_type":"code","execution_count":null,"id":"OeBDRF5rQdCJ","metadata":{"id":"OeBDRF5rQdCJ"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","tb = int(25) # batch size\n","# create DataLoaders train/test\n","train_dataloader = DataLoader(dataset_train, batch_size=tb,shuffle=True)\n","test_dataloader = DataLoader(dataset_test, batch_size=tb,shuffle=False)\n","\n","nbepochs =2\n","loss = nn.CrossEntropyLoss() # cross entropy loss\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","model.train()\n","model.cuda()\n","\n","# TRAINING LOOP\n","for e in range(nbepochs): # LOOP over epochs\n","  for idx,batch in enumerate(train_dataloader): # LOOP over batches\n","    b_input_ids = batch[0].cuda()\n","    b_input_mask = batch[1].cuda()\n","    b_labels = batch[2].cuda()\n","\n","    # TODO: ZERO the gradient accumulator - YOUR CODE HERE\n","    optimizer.zero_grad()\n","    # TODO: Compute prediction (forward pass) - YOUR CODE HERE\n","    pred = model(input_ids=b_input_ids, attention_mask=b_input_mask).logits\n","    # TODO: Compute loss (cross entropy) between predictions and labels - YOUR CODE HERE\n","    l = loss(pred, b_labels)\n","    # TODO: Compute gradients (backward pass) - YOUR CODE HERE\n","    l.backward()\n","    # TODO: update parameters\n","    optimizer.step()\n","\n","  print(\"epoch\",e,\" acc train=\",accuracy(model,train_dataloader),\" acc test=\",accuracy(model,test_dataloader) ) # Computing performances at the end of each epoch\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1S8ijD0HNVpDaNZFvOMiltj7qi_N6jf8Z","timestamp":1707490541145},{"file_id":"1a8Pro5FG8LgO0jtypN_o6vL65YWF5-Zf","timestamp":1676565988413},{"file_id":"1H6b5gJT0mW7AJOAEVxiIN6q2v8fQl_yG","timestamp":1676559453001}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}
