{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME 01 - Bag of Words\n",
    "\n",
    "Auteur : LUONG Ethan\n",
    "Auteur : PHAM Louis-Antoine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import os.path\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRESIDENTS_FILENAME = './datasets/AFDpresidentutf8/corpus.tache1.learn.utf8'\n",
    "MOVIES_DIRNAME = './datasets/movies/movies1000/'\n",
    "\n",
    "# Présidents\n",
    "def load_pres(fname):\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(-1)\n",
    "        else: \n",
    "            alllabs.append(1)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs\n",
    "\n",
    "# Films\n",
    "def load_movies(path2data): # 1 classe par répertoire\n",
    "    alltxts = [] # init vide\n",
    "    labs = []\n",
    "    cpt = 0\n",
    "    for cl in os.listdir(path2data): # parcours des fichiers d'un répertoire\n",
    "        for f in os.listdir(path2data+cl):\n",
    "            txt = open(path2data+cl+'/'+f).read()\n",
    "            alltxts.append(txt)\n",
    "            labs.append(cpt)\n",
    "        cpt+=1 # chg répertoire = cht classe\n",
    "        \n",
    "    return alltxts,labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movtxts, movlabs = load_movies(MOVIES_DIRNAME)\n",
    "\n",
    "print(f'Data size: {len(movtxts)}, {len(movlabs)}')\n",
    "\n",
    "print(f'{movtxts[0][-100:]} {movlabs[0]}')\n",
    "print(f'{movtxts[-1][-100:]} {movlabs[-1]}')\n",
    "\n",
    "classes, counts = np.unique(movlabs, return_counts=True)\n",
    "print(f'Class 0: {counts[0]} examples, Class 1: {counts[1]} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitement des données textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    punc = string.punctuation + '\\n\\t\\r'\n",
    "    text = text.translate(str.maketrans(punc, ' ' * len(punc)))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "\n",
    "    # Stemming\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    text = reduce(lambda x, y: x + \" \" + ps.stem(y), text.split(' '), \"\").strip()\n",
    "\n",
    "    # Remove duplicate whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before preprocessing:\\n{movtxts[100][:100]}\\n')\n",
    "print(f'After preprocessing:\\n{preprocess(movtxts[100][:100])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "\n",
    "stopwords = []\n",
    "for stopword in stopwords_list:\n",
    "    stopwords.append(preprocess(stopword))\n",
    "\n",
    "#processing on stopwords\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "stopwords_stem = [ps.stem(x) for x in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données\n",
    "\n",
    "Reste :\n",
    "<ul>\n",
    "    <li>voc original</li>\n",
    "    <li>fréq. documentaire</li>\n",
    "    <li>odd ratio</li>\n",
    "    <li>n grams</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(alltxts):\n",
    "    V = {}\n",
    "    for text in alltxts:\n",
    "        for word in text.split():\n",
    "            V[word] = 1\n",
    "    return V\n",
    "\n",
    "print(f'|V| = {len(get_vocabulary(movtxts).keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 2, figsize=(15, 12))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "wordcloud = WordCloud(background_color='white', max_words=100, stopwords=stopwords_list).generate(\" \".join(prestxts))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Presidents Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "wordcloud = WordCloud(background_color='white', max_words=100, stopwords=stopwords_list).generate(\" \".join(movtxts))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Movies Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zipf\n",
    "vectorizer_tfidf = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(movtxts)\n",
    "indices = np.argsort(-X_tfidf.toarray().sum(axis=0))\n",
    "plt.plot(X_tfidf.toarray()[:, indices[:50]].sum(axis=0))\n",
    "plt.xticks(range(50), vectorizer_tfidf.get_feature_names_out()[indices[:50]], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles de Machine Learning\n",
    "\n",
    "Reste :\n",
    "<ul>\n",
    "    <li>optimisation mindf et maxdf</li>\n",
    "    <li>bigrammes et trigrammes</li>\n",
    "    <li>comparaisons subplots</li>\n",
    "    <li>avec/sans équilibrage des classes pour les présidents</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problèmes de convergence en régression logistique avec CountVectorizer, performances similaires\n",
    "# vectorizer_count = CountVectorizer(preprocessor=preprocess, stop_words=stopwords)\n",
    "# X_count = vectorizer_count.fit_transform(movtxts)\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords_stem)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(movtxts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cross_val(X, labels, cv=5, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    accuracies = []\n",
    "    roc_auc_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb_clf = MultinomialNB()\n",
    "    accuracies.append(cross_val_score(nb_clf, X, labels, cv=cv).mean())\n",
    "    roc_auc_scores.append(cross_val_score(nb_clf, X, labels, cv=cv, scoring=\"roc_auc\").mean())\n",
    "    f1_scores.append(cross_val_score(nb_clf, X, labels, cv=cv, scoring=\"f1\").mean())\n",
    "    \n",
    "    # Linear SVM\n",
    "    svm_clf = LinearSVC(random_state=0, dual='auto')\n",
    "    accuracies.append(cross_val_score(svm_clf, X, labels, cv=cv).mean())\n",
    "    roc_auc_scores.append(cross_val_score(svm_clf, X, labels, cv=cv, scoring=\"roc_auc\").mean())\n",
    "    f1_scores.append(cross_val_score(svm_clf, X, labels, cv=cv, scoring=\"f1\").mean())\n",
    "\n",
    "    # Logistic Regression\n",
    "    t = 1e-8\n",
    "    C = 10.0\n",
    "    lr_clf = LogisticRegression(random_state=0, solver='liblinear', max_iter=100, tol=t, C=C,class_weight=\"balanced\")\n",
    "    accuracies.append(cross_val_score(lr_clf, X, labels, cv=cv).mean())\n",
    "    roc_auc_scores.append(cross_val_score(lr_clf, X, labels, cv=cv, scoring=\"roc_auc\").mean())\n",
    "    f1_scores.append(cross_val_score(lr_clf, X, labels, cv=cv, scoring=\"f1\").mean())\n",
    "\n",
    "    x_axis = np.arange(3)\n",
    "    width = 0.2\n",
    "    gap = width + 0.05\n",
    "    plt.figure()\n",
    "    plt.title('Cross Validation Scores')\n",
    "    acc_bar = plt.bar(x_axis - gap, accuracies, width=width, label='Accuracy', color='steelblue')\n",
    "    roc_auc_bar = plt.bar(x_axis, roc_auc_scores, width=width, label='ROC AUC', color='mediumseagreen')\n",
    "    f1_bar = plt.bar(x_axis + gap, f1_scores, width=width, label='F1 Score', color='lightcoral')\n",
    "    plt.xticks(x_axis, ['NB', 'SVM', 'LR'])\n",
    "    for rect in acc_bar + roc_auc_bar + f1_bar:\n",
    "        height = rect.get_height()\n",
    "        plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    np.random.seed()\n",
    "\n",
    "test_cross_val(X_tfidf, movlabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_test_split(X, labels, test_size=0.2, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, labels, test_size=test_size)\n",
    "\n",
    "    accuracies = []\n",
    "    roc_auc_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb_clf = MultinomialNB()\n",
    "    nb_clf.fit(X_train, Y_train)\n",
    "    Y_pred = nb_clf.predict(X_test)\n",
    "    accuracies.append(accuracy_score(Y_test, Y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(Y_test, Y_pred))\n",
    "    f1_scores.append(f1_score(Y_test, Y_pred))\n",
    "\n",
    "    # Linear SVM\n",
    "    svm_clf = LinearSVC(random_state=0, dual='auto')\n",
    "    svm_clf.fit(X_train, Y_train)\n",
    "    Y_pred = svm_clf.predict(X_test)\n",
    "    accuracies.append(accuracy_score(Y_test, Y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(Y_test, Y_pred))\n",
    "    f1_scores.append(f1_score(Y_test, Y_pred))\n",
    "\n",
    "    # Logistic Regression\n",
    "    t = 1e-8\n",
    "    C = 100.0\n",
    "    lr_clf = LogisticRegression(random_state=0, solver='liblinear', max_iter=100, tol=t, C=C)\n",
    "    lr_clf.fit(X_train, Y_train)\n",
    "    Y_pred = lr_clf.predict(X_test)\n",
    "    accuracies.append(accuracy_score(Y_test, Y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(Y_test, Y_pred))\n",
    "    f1_scores.append(f1_score(Y_test, Y_pred))\n",
    "\n",
    "    x_axis = np.arange(3)\n",
    "    width = 0.2\n",
    "    gap = width + 0.05\n",
    "    plt.figure()\n",
    "    plt.title('Train-Test Split Scores')\n",
    "    acc_bar = plt.bar(x_axis - gap, accuracies, width=width, label='Accuracy', color='steelblue')\n",
    "    roc_auc_bar = plt.bar(x_axis, roc_auc_scores, width=width, label='ROC AUC', color='mediumseagreen')\n",
    "    f1_bar = plt.bar(x_axis + gap, f1_scores, width=width, label='F1 Score', color='lightcoral')\n",
    "    plt.xticks(x_axis, ['NB', 'SVM', 'LR'])\n",
    "    for rect in acc_bar + roc_auc_bar + f1_bar:\n",
    "        height = rect.get_height()\n",
    "        plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    np.random.seed()\n",
    "\n",
    "test_train_test_split(X_tfidf, movlabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary\n",
    "vectorizer_tfidf_bin = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords_stem, binary=True)\n",
    "X_tfidf_bin = vectorizer_tfidf_bin.fit_transform(movtxts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cross_val(X_tfidf_bin, movlabs)\n",
    "test_train_test_split(X_tfidf_bin, movlabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(X, alllabs, tests):\n",
    "    np.random.seed(0)\n",
    "    svm_clf = LinearSVC(random_state=0, dual='auto')\n",
    "    svm_clf.fit(X, alllabs)\n",
    "    pred = svm_clf.predict(tests)\n",
    "    with open('results.txt', 'w') as f:\n",
    "        for yhat in pred:\n",
    "            if yhat == 1:\n",
    "                f.write('P\\n')\n",
    "            else:\n",
    "                f.write('N\\n')\n",
    "\n",
    "with open('./datasets/movies/testSentiment.txt', 'r', encoding='utf-8') as f:\n",
    "    movtests = f.readlines()\n",
    "\n",
    "X_test = vectorizer_tfidf_bin.transform(movtests)\n",
    "export_data(X_tfidf_bin, movlabs, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test n-grams avec binaire\n",
    "vectorizer_tfidf = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords_stem,ngram_range=(1,2),binary=True)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(movtxts)\n",
    "print(X_tfidf.shape)\n",
    "test_train_test_split(X_tfidf, movlabs)\n",
    "test_cross_val(X_tfidf, movlabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(X, alllabs, tests):\n",
    "    np.random.seed(0)\n",
    "    svm_clf = LinearSVC(random_state=0, dual='auto')\n",
    "    svm_clf.fit(X, alllabs)\n",
    "    pred = svm_clf.predict(tests)\n",
    "    with open('results.txt', 'w') as f:\n",
    "        for yhat in pred:\n",
    "            if yhat == 1:\n",
    "                f.write('P\\n')\n",
    "            else:\n",
    "                f.write('N\\n')\n",
    "\n",
    "with open('./datasets/movies/testSentiment.txt', 'r', encoding='utf-8') as f:\n",
    "    movtests = f.readlines()\n",
    "\n",
    "X_test = vectorizer_tfidf.transform(movtests)\n",
    "export_data(X_tfidf, movlabs, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Présidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pré-traitement pour les documents des discours\n",
    "def preprocess_pres(text):\n",
    "\n",
    "\n",
    "    # Remove punctuation\n",
    "    punc = string.punctuation + '\\n\\t\\r'\n",
    "    text = text.translate(str.maketrans(punc, ' ' * len(punc)))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "\n",
    "    # Stemming\n",
    "    ps = nltk.stem.SnowballStemmer(\"french\")\n",
    "    text = reduce(lambda x, y: x + \" \" + ps.stem(y), text.split(' '), \"\").strip()\n",
    "\n",
    "    # Remove duplicate whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prestxts, preslabs = load_pres(PRESIDENTS_FILENAME)\n",
    "\n",
    "print(f'Data size: {len(prestxts)}, {len(preslabs)}')\n",
    "\n",
    "print(f'{prestxts[0]} {preslabs[0]}')\n",
    "print(f'{prestxts[-1]} {preslabs[-1]}')\n",
    "\n",
    "classes, counts = np.unique(preslabs, return_counts=True)\n",
    "print(f'Miterrand: {counts[0]} examples, Chirac: {counts[1]} examples')\n",
    "\n",
    "print(f'Before preprocessing:\\n{prestxts[100][:100]}\\n')\n",
    "print(f'After preprocessing:\\n{preprocess_pres(prestxts[100][:100])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.SnowballStemmer(\"french\")\n",
    "stopwords_stem = [ps.stem(x) for x in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer tfidf\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(preprocessor=preprocess_pres, stop_words=stopwords_stem)\n",
    "\n",
    "vectorizer_tf = TfidfVectorizer(preprocessor=preprocess_pres, stop_words=stopwords_stem,use_idf=False)\n",
    "\n",
    "vectorizer_bin = TfidfVectorizer(preprocessor=preprocess_pres, stop_words=stopwords_stem,binary=True)\n",
    "\n",
    "vectorizer_tfidf_gram = TfidfVectorizer(preprocessor=preprocess_pres, stop_words=stopwords_stem,ngram_range=(1,3))\n",
    "\n",
    "vectorizer_tfidf_gram_max_df = TfidfVectorizer(preprocessor=preprocess_pres, stop_words=stopwords_stem,ngram_range=(1,3),max_df=2000,max_features=200000)\n",
    "\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(prestxts)\n",
    "X_tf = vectorizer_tf.fit_transform(prestxts)\n",
    "X_bin = vectorizer_bin.fit_transform(prestxts)\n",
    "X_gram = vectorizer_tfidf_gram.fit_transform(prestxts)\n",
    "X_gram_max = vectorizer_tfidf_gram_max_df.fit_transform(prestxts)\n",
    "\n",
    "#TF-IDF\n",
    "test_cross_val(X_tfidf,preslabs)\n",
    "#TF\n",
    "test_cross_val(X_tf,preslabs)\n",
    "#binaire\n",
    "test_cross_val(X_bin,preslabs)\n",
    "#n_gram 1-3\n",
    "test_cross_val(X_gram,preslabs)\n",
    "\n",
    "#n-gram 1-3 avec réduction des features avec 200000\n",
    "test_cross_val(X_gram_max,preslabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balancement des données\n",
    "idx_mitterrand = np.argwhere(np.array(preslabs) == -1)\n",
    "\n",
    "txt_mitterrand = [prestxts[int(i)] for i in idx_mitterrand]\n",
    "\n",
    "txt_mitterrand_dup = txt_mitterrand*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on duplique les données de la classe minoritaire pour faire le balancement des données\n",
    "print(len(txt_mitterrand_dup))\n",
    "\n",
    "prestxts_dup = prestxts + txt_mitterrand_dup\n",
    "preslabs_dup = preslabs + list(-np.ones(len(txt_mitterrand_dup),dtype = int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_dup = vectorizer_tfidf.fit_transform(prestxts_dup)\n",
    "X_tf_dup = vectorizer_tf.fit_transform(prestxts_dup)\n",
    "X_bin_dup = vectorizer_bin.fit_transform(prestxts_dup)\n",
    "X_gram_dup = vectorizer_tfidf_gram.fit_transform(prestxts_dup)\n",
    "\n",
    "\n",
    "test_cross_val(X_tfidf_dup,preslabs_dup)\n",
    "test_cross_val(X_tf_dup,preslabs_dup)\n",
    "test_cross_val(X_bin_dup,preslabs_dup)\n",
    "test_cross_val(X_gram_dup,preslabs_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(size):\n",
    "    sigma = size/3\n",
    "    x = np.arange(-size, size+1)\n",
    "    kern =  np.exp(-(x**2)/(2*sigma*sigma))\n",
    "    return kern / kern.sum()\n",
    "\n",
    "def gaussian_smoothing(pred, size):\n",
    "    predictions = np.copy(pred)\n",
    "    kernel = gaussian_kernel(size)\n",
    "    return np.convolve(predictions, kernel, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(list(range(len(preslabs[0:100]))), preslabs[0:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(list(range(len(preslabs[0:100]))), gaussian_smoothing(preslabs[0:100], 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export des prédiction pour les discours\n",
    "def export_data_pres(X, alllabs, tests,clf):\n",
    "    np.random.seed(0)\n",
    "    clf.fit(X, alllabs)\n",
    "    pred = clf.predict_proba(tests)\n",
    "    pred = gaussian_smoothing(pred[:,0],3) #on fait le post-traitement dans l'export\n",
    "    with open('results_pres.txt', 'w') as f:\n",
    "        for yhat in pred:\n",
    "            f.write(f'{yhat}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRESIDENTS_FILENAME_TEST = './datasets/AFDpresidentutf8/corpus.tache1.test.utf8'\n",
    "\n",
    "def load_pres_test(fname):\n",
    "    alltxts = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*>(.*)\",\"\\\\1\",txt)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts\n",
    "text_test = load_pres_test(PRESIDENTS_FILENAME_TEST)\n",
    "\n",
    "print(text_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(random_state=0, solver='liblinear', max_iter=100, tol=1e-8, C=10)\n",
    "X_tfidf_test = vectorizer_tfidf_gram.transform(text_test)\n",
    "export_data_pres(X_gram_dup,preslabs_dup,X_tfidf_test,lr_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(random_state=0, solver='liblinear', max_iter=100, tol=1e-8, C=10,class_weight=\"balanced\")\n",
    "X_tfidf_test = vectorizer_tfidf_gram_max_df.transform(text_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
