{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data mining \\& clustering\n",
    "\n",
    "The goal if this practical is to adress the folowing problem: \n",
    "<center style=\"color:red\" >  Given XXX raw, unlabeled documents, ... How to exploit/understand/represent them?</center>\n",
    "\n",
    "In the previous week, we have seen how to represent textual data with the Bag of Words (BoW) model:\n",
    "$$X = \n",
    "\t\\begin{matrix} \n",
    "\t & \\textbf{t}_j \\\\\n",
    "\t & \\downarrow \\\\\n",
    "\t\\textbf{d}_i \\rightarrow &\n",
    "\t\\begin{pmatrix} \n",
    "\tx_{1,1} & \\dots & x_{1,d} \\\\\n",
    "\t\\vdots & \\ddots & \\vdots \\\\\n",
    "\tx_{N,1} & \\dots & x_{N,d} \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\end{matrix}\n",
    "\t$$\n",
    "\n",
    "From this BoW representation, we want to answer the following questions:\n",
    "1. Which clustering algorithm to choose?\n",
    "    - K-means, LSA, pLSA, LDA\n",
    "1. What results to expect?\n",
    "    - Semantics, noise cleaning, etc...\n",
    "1. Which qualitative and quantitative analyses to understand the groups?\n",
    "[comment]: <> (%1. Comment boucler, itérer pour améliorer la qualité du processus?)\n",
    "\n",
    "\n",
    "<span style=\"color:magenta\" > In this practical, we use a **labeled dataset** in order to evaluate performances with quantitative and well-defined metrics. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import os.path\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion BoW + tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#vectorizer = TfidfVectorizer() \n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(vectors.shape)\n",
    "\n",
    "# sparsity measure = 44 active words over 1000 per document (157 over 130000) !!\n",
    "print(vectors.nnz / float(vectors.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve words\n",
    "print([(i,vectorizer.get_feature_names_out()[i]) \\\n",
    "       for i in np.random.randint(vectors.shape[1], size=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels (only for evaluation)\n",
    "Y = newsgroups_train.target\n",
    "print(Y[:10]) \n",
    "print([newsgroups_train.target_names[i] for i in Y[:20]]) # vraie classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Word clouds \n",
    "### Drawing word clouds from the raw corpus or words' frequencies :  [make word clouds !](https://github.com/amueller/word_cloud)\n",
    "\n",
    "### Installation\n",
    "If you are using pip:\n",
    "\n",
    "`pip install wordcloud`\n",
    "\n",
    "### If you are using conda, you can install from the conda-forge channel:\n",
    "\n",
    "`conda install -c conda-forge wordcloud`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the most frequent words in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(newsgroups_train.data)\n",
    "corpus = \"\".join(data)\n",
    "words = corpus.split() # optional args to choose the splitting chars\n",
    "print(\"Nb mots=\",len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the N frequent words and verify that its follows a Zipf law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'|V| = {len(vectorizer.vocabulary_.keys())}')\n",
    "indices = np.argsort(-vectors.toarray().sum(axis=0))\n",
    "plt.plot(vectors.toarray()[:, indices[:50]].sum(axis=0))\n",
    "plt.xticks(range(50), vectorizer.get_feature_names_out()[indices[:50]], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color='white', stopwords = [], max_words=100).generate(corpus)            \n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS # Note: this is the default option           \n",
    "wordcloud = WordCloud(background_color='white', stopwords = STOPWORDS, max_words=100).generate(corpus) \n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use word clouds with generate\\_from\\_frequencies. \n",
    "N.B.: retrieve the most words frequencies using a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "indices = np.argsort(-vectors.toarray().sum(axis=0))\n",
    "for word in vectorizer.get_feature_names_out()[indices[:50]]:\n",
    "    d[word] = vectors.toarray()[:,indices[:50]].sum()\n",
    "\n",
    "wordcloud = WordCloud(background_color='white', max_words=100).generate_from_frequencies(d)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing word clouds from classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "plt.subplots(4, 5, figsize=(30,15))\n",
    "i = 1\n",
    "for cl in np.unique(Y):\n",
    "    data = np.array(newsgroups_train.data)[Y==cl]\n",
    "    plt.subplot(4, 5, i)\n",
    "    plt.title(f'Class {cl}: {newsgroups_train.target_names[cl]}')\n",
    "    i += 1\n",
    "    wordcloud = WordCloud(background_color='white', max_words=100).generate(\" \".join(data))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Clustering algorithm: K-Means\n",
    "\n",
    "**Let's start by the most famous and simple unsupervised algorithm: $k$-means!**\n",
    "Look at [sklear documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "and apply it to your BoW matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# your code here\n",
    "kmeans = KMeans(n_clusters=20, random_state=0, max_iter=10).fit(vectors)\n",
    "# Getting clusters:\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(4, 5, figsize=(30,15))\n",
    "i = 1\n",
    "for cl in np.unique(kmeans.labels_):\n",
    "    plt.subplot(4, 5, i)\n",
    "    i += 1\n",
    "    indices_cl = np.where(kmeans.labels_ == cl)[0]\n",
    "    v = vectors.toarray()[indices_cl,:]\n",
    "    indices = np.argsort(-v.sum(axis=0))\n",
    "    d = {}\n",
    "    for word in vectorizer.get_feature_names_out()[indices[:50]]:\n",
    "        d[word] = v[:, indices[:50]].sum()\n",
    "    wordcloud = WordCloud(background_color='white', max_words=100).generate_from_frequencies(d)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(f'Class {cl}')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Analysis: \n",
    "1. **Qualtitative:**\n",
    "    - Look at the most important words for each cluster\n",
    "    - Perform cluster assignement to each document, and compute word cloud on the document (raw text or frequencies)\n",
    "2. **Quantitative:**\n",
    "    - Compute cluster \"purity\": $p_j= |y^*_j|$, where $y^*_j$ is the most frequent (GT) label in cluster $C_j$ $\\Rightarrow$ $p = \\frac{1}{N}\\sum\\limits_j  p_j$\n",
    "    - Compute [Rand Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html) and [Adjusted Rand Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Latent Semantic Analysis (LSA <=> SVD) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember the LSA factorziation**: \n",
    "$$ \n",
    "\\begin{matrix} \n",
    " & X  &\\!\\!\\!\\!\\!=\\!\\!\\!\\!\\!& U  & \\Sigma & V^T \\\\\n",
    "  & \\textbf{t}_j   &  & \\hat{ \\textbf{d}_i} & &  \\\\\n",
    " & \\downarrow  &  &\\downarrow  & & \\\\\n",
    "\\textbf{d}_i \\rightarrow \n",
    "&\n",
    "\\begin{pmatrix} \n",
    "x_{1,1} & \\dots & x_{1,d} \\\\\n",
    "\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\\\\n",
    "x_{N,1} & \\dots & x_{N,d} \\\\\n",
    "\\end{pmatrix}\n",
    "&\n",
    "\\!\\!\\!\\!\\!=\\!\\!\\!\\!\\!\n",
    "%&\n",
    "%(\\hat{ \\textbf{t}_j}) \\rightarrow\n",
    "&\n",
    "\\begin{pmatrix} \n",
    "\\begin{pmatrix} &  \\textbf{u}_1 &  \\end{pmatrix} \\\\\n",
    "\\vdots \\\\\n",
    "\\begin{pmatrix}  & \\textbf{u}_k &  \\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "%&\n",
    "%\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\n",
    "&\n",
    "\\begin{pmatrix} \n",
    "\\sigma_1 & \\dots & 0 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\dots & \\sigma_k \\\\\n",
    "\\end{pmatrix}\n",
    "%&\n",
    "%\\!\\!\\!\\!\\!\\cdot\\!\\!\\!\\!\\!\n",
    "&\n",
    "\\begin{pmatrix} \n",
    "\\begin{pmatrix} \\, \\\\ \\, \\\\ \\textbf{v}_1 \\\\ \\, \\\\ \\,\\end{pmatrix} \n",
    "\\dots\n",
    "\\begin{pmatrix} \\, \\\\ \\, \\\\ \\textbf{v}_k \\\\ \\, \\\\ \\, \\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at [SVD doc in skelarn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD)\n",
    "- Do the same qualitative/quantitative evaluation than with K-Means\n",
    "- You can also use LSA as a pre-processing step for K-Means, *i.e.* running K-Means on $\\boldsymbol{U}$ matrix above\n",
    "    - N.B. : try without/with $\\ell_2$ normalization of $\\boldsymbol{U}$'s rows before running  K-Means\n",
    "    - You can also benefit from LSA pre-processing for using [t-SNE visualization](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) (see code below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "U = svd.fit_transform(vectors.toarray())\n",
    "vectors_SVDn = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE from the U matrix computed by LSA\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, init='pca',n_iter=5000)\n",
    "tsne_mat = tsne.fit_transform(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2cluster = np.argmax(np.abs(vectors_SVDn), axis=0)\n",
    "#import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "cmap = cm.tab20\n",
    "cmap = cm.get_cmap('hsv', 20) \n",
    "cmap = cm.get_cmap('jet', 20)\n",
    "#cmap = cm.tab20\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(tsne_mat[:,0],tsne_mat[:,1], c=Y, cmap=cmap, s=10)\n",
    "plt.scatter(tsne_mat[NN2cluster[:],0],tsne_mat[NN2cluster[:],1], c='black', s=100)\n",
    "#plt.scatter(tsne_mat[NN2cluster2[:],0],tsne_mat[NN2cluster2[:],1], c='red', s=100)\n",
    "plt.colorbar(ticks=range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same experiments with LDA:\n",
    "- LDA\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start with a CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        ngram_range = (1,1),\n",
    "                        tokenizer = tokenizer.tokenize, max_df=0.95, min_df=2, max_features=1000)\n",
    "\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(vectors.shape)\n",
    "print(vectors.nnz / float(vectors.shape[0]))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=20)\n",
    "lda.fit(vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA-viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install install pyldavis\n",
    "from __future__ import print_function\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "\n",
    "pyLDAvis.lda_model.prepare(lda,vectors,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performances evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the different approaches wrt three quantitative metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix = sklearn.metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_kmeans = kmeans.predict(vectors)\n",
    "print('Kmeans Rand Score:', sklearn.metrics.rand_score(Y, Y_pred_kmeans))\n",
    "print('Kmeans Adjusted Rand Score:', sklearn.metrics.adjusted_rand_score(Y, Y_pred_kmeans))\n",
    "print('Kmeans Purity Score:', purity_score(Y, Y_pred_kmeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_tsne = KMeans(n_clusters=20)\n",
    "Y_pred_tsne = kmeans_tsne.fit_predict(tsne_mat)\n",
    "print('t-SNE Rand Score:', sklearn.metrics.rand_score(Y, Y_pred_tsne))\n",
    "print('t-SNE Adjusted Rand Score:', sklearn.metrics.adjusted_rand_score(Y, Y_pred_tsne))\n",
    "print('t-SNE Purity Score:', purity_score(Y, Y_pred_tsne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lda = lda.transform(vectors).argmax(axis=1)\n",
    "print('LDA Rand Score:', sklearn.metrics.rand_score(Y, Y_pred_lda))\n",
    "print('LDA Adjusted Rand Score:', sklearn.metrics.adjusted_rand_score(Y, Y_pred_lda))\n",
    "print('LDA Purity Score:', purity_score(Y, Y_pred_lda))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
